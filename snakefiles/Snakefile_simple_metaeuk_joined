# This is the pipeline to run MetaEuk from beginning (given sample accession) without any kraken filtering and bowtie mapping

# Compared to Snakefile_simple here we wisely concatenate all EukRep contigs and running Metaeuk on all of them together. Should speed up.

# IMPORTANT: ~/EukBook/MinIO_secret.sh must be sourced before Snakemake execution!






############################################
###                                      ###
###             Configuration            ###
###                                      ###
############################################

# Main arguments to the Megahit Snakemake pipeline must be passed with a config.

#### Required input config parameters: ####
# (must be passed with --configafile from yaml or json file, or in Snakemake command with --config)
# (accessed in current snakefile with config['PARAMETER'])
#
#  * SAMPLE    - list of desired sample accessions to obtain MetaEuk predictions for.
#  * SAMPLESET - unique name for the set of selected accesions. 
#                Will be used to create a local subfolder and remote MetaEuk results folder.

###  Example config usage
#   snakemake [other arguments] --config SAMPLE=SRR2844600 SAMPLESET=set_07 
#   snakemake [other arguments] --configfile path/to/configfile.yaml
#       Example configfile.yaml:
#            SAMPLESET:
#               - set1
#            SAMPLE:
#               - SRR5918406
#               - SRR5788362
#               - SRR5788367
#               - ...
#
#       NOTE: --config can pass only one string value for a parameter, e.g. only one sample.
#             Passing multiple samples is possible only with config file. 
#       NOTE: If both --config and --configfile are used and specify parameter with same name, 
#             --config parameter will overwrite the one passed from --configfile.
#             Parameters with different names will end up in final config independently 
#             of way they were specified.








####   Hardcoded parameters:           ####
mainfolder='/home/ubuntu/EukBook'
maxthreads = snakemake.utils.available_cpu_count()
megahit_version = "megahit_v1.2.9" # Snakemake will create this remote folder for megahit results.
                                   # Prevents overwriting if different version of assembler is used.

# The batch number. (Will be deprecated) It is the subfolder in our remote MinIO container,
# where the results are saved. A new batch makes new calculations for samples,
# that were already calculated for earlier batch. This is made to treat new versions
# of pipeline separately and don`t override old results.
BATCH="batch2" 



## Benchmarking mode?
#benchmark=True
benchmark=False

if (benchmark==False):
    downloading_script="download_fastq.sh"
    mincontiglen=5000
    EUK_PROFILES="/mnt/metaeuk_profiles/metaeuk_ref_profiles_more_than1_MMETSP_zenodo_3247846_profiles"
    BATCH=BATCH
    megahit_version=megahit_version
else:
    downloading_script="download_fastq_short.sh" # subsets only first 10k reads
    mincontiglen=100 #otherwise no contigs left, too few reads for 5000kb+ contigs
    EUK_PROFILES="/mnt/metaeuk_profiles_short/MMETSP_uniclust50_MERC_profiles_more_than1_few"
    BATCH="batch_benchmark"
    megahit_version="megahit_benchmark"






# Attaching the remote S3 server for storing pipeline results.
# IMPORTANT: Global credential variables should be specified in bash environment 
#            before running current Snakefile, for example by sourcing a script.
#            Example script:
#              MinIO_secret.sh
#
#           export MINIO_HOST="https://openstack.cebitec.uni-bielefeld.de:8080"
#           export MINIO_ID="<secret-key-id>"
#           export MINIO_SECRET="<secret-key>"
import os
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host=os.environ["MINIO_HOST"],
                      access_key_id=os.environ["MINIO_ID"],
                      secret_access_key=os.environ["MINIO_SECRET"])

# de.nbi bielefeld container that stores a copy of metagenomes from ENA 
ena = S3RemoteProvider(host="https://openstack.cebitec.uni-bielefeld.de:8080",
                       access_key_id="",
                       secret_access_key="")







#### Handling different input ways of SAMPLESET config. ####
# Normally SAMPLESET should be passed with configfile together with corresponding accessions.
#    In this case yaml always passes an array, even if only with one value.
#    So, to obtain the actual sampleset string value, need to take first element of such array input.
# But if the sampleset is explicitly specified with --config SAMPLESET="set_somethin",
#   then it is already a string and if we are taking first element we will get only the first character.
if type(config["SAMPLESET"]) is list:
    setn=config["SAMPLESET"][0]
elif type(config["SAMPLESET"]) is str:
    setn=config["SAMPLESET"]







####     Email notifications     ####
targetmail="dembra96@gmail.com"

onstart:
    if (benchmark==False):
        print("Workflow finished, no error")
        shell("mail -s 'Snakemake "+setn+" - Workflow started' %s < {log}" % targetmail)

onsuccess:
    if (benchmark==False):
        print("Workflow finished, no error")
        shell("mail -s 'Snakemake "+setn+" - Workflow successfully finished' %s < {log}" % targetmail)

onerror:
    if (benchmark==False):
        print("An error occurred")
        shell("mail -s 'Snakemake "+setn+" - an error occurred' %s < {log}" % targetmail)








############################################
###                                      ###
###           Input functions            ###
###                                      ###
############################################

# Helper function to list files on remote server via MinIO client.
#  Gives a python list of file names found in the path by pattern.
import subprocess

def mc_ls(path="eukupload/", pattern=""):
    mc_raw_output = str(subprocess.check_output(
                             ("mc ls "+path+" | " + 
                              "awk '{print $5}' | " + 
                              "awk '/"+pattern+"/ {print}'"),
                                                shell=True)
                       )
    mc_file_list = mc_raw_output.strip('b').strip("'").split("\\n")
    return mc_file_list

# Input function to obtain megahit results
def get_megahit_source(sample):
    #sample = "{wildcards.sample}".format(wildcards=wildcards)
    if (os.path.isfile(setn+"/%s/contigs.megahit.fa" % sample)):
        print("Megahit %s assembly was found locally and will be used." % sample)
        output = setn+"/%s/contigs.megahit.fa"%sample
    else:
        remote_megahits = mc_ls(path="eukupload/eukcontainer/contigs/"+megahit_version, pattern="contigs\.megahit\.fa\.tar\.gz$")
        if ("%s.contigs.megahit.fa.tar.gz"%sample in remote_megahits):
            print("Remote copy of Megahit assembly %s was found and will be downloaded." % sample)
            output = setn+"/%s/downloaded_contigs.megahit.fa"%sample
        else:
            print("Atention! No Megahit assembly for %s was found on a remote server or in local folder, so it will be performed de novo." % sample)
            output = setn+"/%s/contigs.megahit.fa"%sample
    return output








############################################
###                                      ###
###               Workflow               ###
###                                      ###
############################################


# Rule that determines required output of pipeline
rule all:
    input:
        report = S3.remote("eukcontainer/"+BATCH+"/"+ setn+".reports.tar.gz"),
        main_archive = S3.remote("eukcontainer/"+BATCH+"/"+ setn+".tar.gz"),
        sample_list = S3.remote("eukcontainer/"+BATCH+"/"+ setn+"_samples.txt"),



#compress all needed files (will be deprecated)
rule tar_big_archive:
    input: 
        ##REPORTS
        reports = [
        #FastQC
        expand(setn+"/{sample}/reads_{n}{trim}_fastqc.{ext}", 
               sample=config["SAMPLE"], n=["1","2"], trim=["","_trim"], ext=["html","zip"]),
        #Fastp
        expand(setn+"/{sample}/fastp.html", sample=config["SAMPLE"]),
        expand(setn+"/{sample}/fastp.json", sample=config["SAMPLE"]),
        #QUAST
        #setn+"/quast_multi_raw",
        setn+"/quast_multi_filt_raw",
        setn+"/quast_multi_EukRep", 
        #MultiQC
        setn+"/multiqc_report.html",
        setn+"/multiqc_data",
        #logs
        setn+"/log_MetaEuk_easypredict.txt",
        #expand(setn+"/{sample}/log_megahit.txt", sample=config["SAMPLE"]),
        expand(setn+"/{sample}/log_EukRep.txt", sample=config["SAMPLE"])],
        
        ## RESULTS
        #Megahit
        #megahit = expand(setn+"/{sample}/contigs.megahit.fa", sample=config["SAMPLE"]),
        #EukRep
        eukrep = expand(setn+"/{sample}/contigs.EukRep.fa", sample=config["SAMPLE"]),
        #MetaEuk
	metaeuk = [setn+"/MetaEuk_preds.fasta",
                   setn+"/MetaEuk_preds.fasta.headersMap.tsv"],
    output:
        main = S3.remote("eukcontainer/"+BATCH+"/"+ setn+".tar.gz"),
    shell:
        "tar -cf {output.main} -I 'pigz -p 14' {input} {setn}/tempFolder/latest/{{contigs*,MetaEuk_preds.*}}"

rule tar_reports: #(will be deprecated)
    input:
        ##REPORTS
        reports = [
        #FastQC
        expand(setn+"/{sample}/reads_{n}{trim}_fastqc.{ext}",
               sample=config["SAMPLE"], n=["1","2"], trim=["","_trim"], ext=["html","zip"]),
        #Fastp
        expand(setn+"/{sample}/fastp.html", sample=config["SAMPLE"]),
        expand(setn+"/{sample}/fastp.json", sample=config["SAMPLE"]),
        #QUAST
        #setn+"/quast_multi_raw",
        setn+"/quast_multi_filt_raw",
        setn+"/quast_multi_EukRep",
        #MultiQC
        setn+"/multiqc_report.html",
        setn+"/multiqc_data",
        #logs
        setn+"/log_MetaEuk_easypredict.txt",
        #expand(setn+"/{sample}/log_megahit.txt", sample=config["SAMPLE"]),
        expand(setn+"/{sample}/log_EukRep.txt", sample=config["SAMPLE"])],
    output:
        report = S3.remote("eukcontainer/"+BATCH+"/"+ setn+".reports.tar.gz"),
    shell:
        "tar -czf {output.report} {input.reports}"



#rule to save sample accessions provided in yaml as simple text list to copy to remote container.
rule write_samples:
    output:
        sample_list = S3.remote("eukcontainer/"+BATCH+"/"+ setn+"_samples.txt"),
    run:
        with open(output[0], 'w') as f:
            for item in config["SAMPLE"]:
                f.write("%s\n" % item)


    
### rule to download Protein profiles for MetaEuk if absent
# normally would be run only once with the very first snakemake launch on a new machine
rule GET_METAEUK_PROTEIN_PROFILES:
    output:
        "/mnt/metaeuk_profiles/metaeuk_ref_profiles_more_than1_MMETSP_zenodo_3247846_profiles"
    shell:
        "if mc cp --recursive eukupload/eukcontainer/metaeuk_profiles /mnt/ ; "
        "then echo 'Protein profiles successfully copied from eukcontainer mc copy.'; "
        "else wget http://wwwuser.gwdg.de/~elevyka/metaeuk_ref_more_than1_MMETSP_zenodo_3247846_profiles.tar.gz; "
        "tar -C /mnt/metaeuk_profiles -xzf metaeuk_ref_more_than1_MMETSP_zenodo_3247846_profiles.tar.gz; "
        "rm metaeuk_ref_more_than1_MMETSP_zenodo_3247846_profiles.tar.gz; "
        "echo 'Protein profiles successfully downloaded from gwdg server.'; "
        "fi; "

rule GET_METAEUK_PROTEIN_PROFILES_short:
    output:
        "/mnt/metaeuk_profiles_short/MMETSP_uniclust50_MERC_profiles_more_than1_few"
    shell:
        "if mc cp --recursive eukupload/eukcontainer/metaeuk_profiles_short /mnt/ ; "
        "then echo 'Protein profiles successfully copied from eukcontainer mc copy.'; "
        "else wget http://wwwuser.gwdg.de/~elevyka/MMETSP_uniclust50_MERC_profiles_more_than1_few.tar.gz; "
        "tar -C /mnt/metaeuk_profiles_short -xzf MMETSP_uniclust50_MERC_profiles_more_than1_few.tar.gz; "
        "rm MMETSP_uniclust50_MERC_profiles_more_than1_few.tar.gz; "
        "echo 'Protein profiles successfully downloaded from gwdg server.'; "
        "fi; "



### Preprocessing
rule download_fastq:
    output: 
        setn+"/{sample}/reads_1.fastq.gz",
        setn+"/{sample}/reads_2.fastq.gz"
    shell: 
        "{mainfolder}/scripts/" + downloading_script + " {wildcards.sample} {output}" 


rule FastQC_untrimmed:
    input:
        setn+"/{sample}/reads_1.fastq.gz",
        setn+"/{sample}/reads_2.fastq.gz"
    output:
        expand(setn+"/{{sample}}/reads_{n}_fastqc.{ext}", n=["1","2"], ext=["html","zip"]),
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    threads: 2
    shell: 
        "fastqc -t {threads} {input} "

        
rule fastp_trimming:
    input: 
        setn+"/{sample}/reads_1.fastq.gz",
        setn+"/{sample}/reads_2.fastq.gz"
    output: 
        setn+"/{sample}/reads_1_trim.fastq.gz",
        setn+"/{sample}/reads_2_trim.fastq.gz",
        setn+"/{sample}/fastp.json",
        setn+"/{sample}/fastp.html" 
    threads: 4  
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    shell: 
        "fastp -r -w {threads} -i {input[0]} -I {input[1]} -o {output[0]} -O {output[1]} -j {output[2]} -h {output[3]}"


rule FastQC_trimmed:
    input:
        setn+"/{sample}/reads_1_trim.fastq.gz",
        setn+"/{sample}/reads_2_trim.fastq.gz"
    output:
        expand(setn+"/{{sample}}/reads_{n}_trim_fastqc.{ext}", n=["1","2"], ext=["html","zip"]),
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    threads: 2
    shell:
        "fastqc -t {threads} {input} "




### Assembly without filtering

rule megahit_unfiltered:
    input:
        setn+"/{sample}/reads_1_trim.fastq.gz",
        setn+"/{sample}/reads_2_trim.fastq.gz"
    output:
        directory(setn+"/{sample}/megahit_unfiltered"), #{output[0]}
        setn+"/{sample}/contigs.megahit.fa", #{output[1]} #need to explicitly write in tar operation
        S3.remote("eukcontainer/contigs/"+megahit_version+"/{sample}.contigs.megahit.fa.tar.gz"), #{output[2]}
    log:
        setn+"/{sample}/log_megahit.txt" #need to explicitly write in tar operation
    conda:
        mainfolder + "/yaml/megahit.yaml"
    threads: 7 #But for Megahit I specify -t 14 .
               # This will allow two megahits run in parallell and make sure all cores are 100% busy all the time, 
               # because with single megahit on all cores they are ~75% occupied on average.
    shell:
        "touch {output[2]} ; " # IMPORTANT! A stamp to reserve the temp local folder,
                              # so no finished meghit processes wil delete it before all others also finish. 
        "{{ time megahit -t 14 -1 {input[0]} -2 {input[1]} -o {output[0]} --min-contig-len 500; }} 2> {log}; "
        "cp {output[0]}/final.contigs.fa {output[1]}; " # copy results one folder up for proper QUAST labeling later
        "tar -cf {output[2]} -I 'pigz' -C "+setn+" {wildcards.sample}/contigs.megahit.fa {wildcards.sample}/log_megahit.txt; "
        "touch {setn}/{wildcards.sample}/checkpoint_{wildcards.sample}_is_done.txt" 


# If the backup copies were detected on remote server, they will be used instaed of running new Megahit. 
rule megahit_download:
    input:
        S3.remote("eukcontainer/contigs/"+megahit_version+"/{sample}.contigs.megahit.fa.tar.gz")
    output:
        setn+"/{sample}/downloaded_contigs.megahit.fa",
    shell:
        "mkdir -p {setn} ; "
        "tar -xf {input} -C {setn} ; "
        "mv {setn}/{wildcards.sample}/contigs.megahit.fa {output}"



rule raw_contigs_size_filtering:
    input:
        get_megahit_source
    output:
        setn+"/{sample}/contigs.megahit.sizefiltered.fa"
    shell:
        'awk \'BEGIN {{FS = "\\t" ; OFS = "\\n"}} {{header = $0 ; getline seq ; '
        'if (length(seq) >= '+str(mincontiglen)+') {{print header, seq}}}}\' < {input} > {output}; '
## ALWAYS escape { } \ by {{ }} \\ in snakemake, if you want to pass { } \ to a command, such as awk 




rule EukRep:
    input:
        setn+"/{sample}/contigs.megahit.sizefiltered.fa"
    output:
        setn+"/{sample}/contigs.EukRep.fa"
    log:
        setn+"/{sample}/log_EukRep.txt"
    shell:
        "{{ time EukRep -i {input} -o {output} -m lenient --min "+str(mincontiglen)+"; }} 2> {log} ; "




# QUAST

rule QUAST_raw_contigs:
    input:
        get_megahit_source
    output:
        directory(setn+"/quasts/quast_raw_{sample}")
    conda:
        mainfolder + "/yaml/quast.yaml"
    threads: 4
    shell:
        #"time quast -t {threads} --silent -o {output} {input}"
        "quast -t {threads} --silent --min-contig "+str(mincontiglen)+" -o {output} {input}"

rule QUAST_multisample_raw_contigs_sizefiltered:
    input:
        expand("{setn}/{sample}/contigs.megahit.sizefiltered.fa", sample=config["SAMPLE"],setn=setn)
    output:
        directory(setn+"/quast_multi_filt_raw")
    conda:
        mainfolder + "/yaml/quast.yaml"
    threads: 4
    shell:
        "quast -t {threads} --silent --min-contig "+str(mincontiglen)+" -o {output} {input}; "

rule QUAST_multisample_EukRep:
    input:
        expand("{setn}/{sample}/contigs.EukRep.fa", sample=config["SAMPLE"],setn=setn)
    output:
        directory(setn+"/quast_multi_EukRep")
    conda:
        mainfolder + "/yaml/quast.yaml"
    threads: 4
    shell: 
        #"time quast -t {threads} --silent -o {output} {input}"
        "quast -t {threads} --silent --min-contig "+str(mincontiglen)+" -o {output} {input}"



# Concatenating all EukRep contigs
rule contig_concatenating:
    input:
        expand("{setn}/{sample}/contigs.EukRep.fa", sample=config["SAMPLE"],setn=setn)
    output:
        setn+"/EukRep_contigs_renamed_cat.fasta"
    shell:
        mainfolder + "/scripts/contig_rename_concat.sh {output} {input}" 


# MetaEuk 
rule MetaEuk_easy_predict:
    input:
        setn+"/EukRep_contigs_renamed_cat.fasta", #{input[0]}
        EUK_PROFILES        #{input[1]}
    output:
        setn+"/MetaEuk_preds.fasta",        #{output[0]}
        directory(setn+"/tempFolder"), #{output[1]}
        setn+"/MetaEuk_preds.fasta.headersMap.tsv", #just need to indicate it here so tar rule knows where to find it. 
                                                    #no need to specify it in metaeuk command.
    log:
        setn+"/log_MetaEuk_easypredict.txt"
    threads: maxthreads
    shell:
        "{{ time metaeuk easy-predict {input[0]} {input[1]} {output[0]} {output[1]}"
        " --metaeuk-eval 0.0001 -e 100 --slice-search --min-ungapped-score 35"
        " --min-length 40 --min-exon-aa 20 --metaeuk-tcov 0.6; }} > {log} 2>&1"


# MultiQC
rule MultiQC:
    input:
        expand(setn+"/{sample}/{file}", sample=config["SAMPLE"], file=['reads_1_fastqc.html', 'reads_2_fastqc.html', 'reads_1_trim_fastqc.html', 'reads_2_trim_fastqc.html', 'fastp.html']),
        expand(setn+"/quasts/quast_{stage}_{sample}", stage=["raw"], sample=config['SAMPLE']),
        expand(setn+"/{file}", file=['quast_multi_EukRep', 'quast_multi_filt_raw'])
    output:
        setn+"/multiqc_report.html",
        directory(setn+"/multiqc_data")
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    shell:
        "multiqc -d -f -o "+setn+" "+setn+"/"
# -d option makes multiqc analyse all samples with same names. In our case all samples are reads_1/2, so this is crucial
# -f option makes force overwrite of previous multiqc output. Important for Snakemake reruns.
# -o specifies outdir of output files
