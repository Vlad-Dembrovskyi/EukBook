# This is the pipeline to run MetaEuk from beginning (given sample accession) without any kraken filtering and bowtie mapping

# Compared to Snakefile_simple here we wisely concatenate all EukRep contigs and running Metaeuk on all of them together. Should speed up.

# IMPORTANT: ~/EukBook/MinIO_secret.sh must be sourced before Snakemake execution!






############################################
###                                      ###
###             Configuration            ###
###                                      ###
############################################

# Main arguments to the Megahit Snakemake pipeline must be passed with a config.

configfile: "/home/ubuntu/EukBook/config/config.yaml"
# Additionally, samples.yaml file should by provided with --configfile command line option.

import os
snakemake_folder = os.getcwd()

workdir: config['workdir'] #all files and conda env are created here, can be overwritten with command line option -d

megahit_version   = config['megahit_version']
EukRep_version    = config['EukRep_version']
EukRep_mode       = config['EukRep_mode']
MetaEuk_version   = config['MetaEuk_version']
MetaEuk_reference = config['MetaEuk_reference']
MetaEuk_taxAnnot  = config['MetaEuk_taxAnnot']
MetaEuk_taxAnnot_small = "seqTaxOnlyCharoDB"

Quick_mode        = config['Quick_mode']
Megahit_parallel_mode = config['Megahit_parallel_mode']




## Handling different input ways of SAMPLESET config. ##
# Normally SAMPLESET should be passed with configfile together with corresponding accessions.
#    In this case yaml always passes an array, even if only with one value.
#    So, to obtain the actual sampleset string value, need to take first element of such array input.
# But if the sampleset is explicitly specified with --config SAMPLESET="set_somethin",
#   then it is already a string and if we are taking first element we will get only the first character.
if type(config["SAMPLESET"]) is list:
    setn=config["SAMPLESET"][0]
elif type(config["SAMPLESET"]) is str:
    setn=config["SAMPLESET"]



###########################
##      Quick mode?      ##
###########################

#### Quick mode: Changing Snakemake behavior to quickly run through using micro-subsetted files ####
if (Quick_mode==False):
    downloading_script="download_fastq.sh"
    mincontiglen=5000
    EUK_PROFILES="/mnt/metaeuk_profiles/"+ MetaEuk_reference
    EUK_taxAnnot="/mnt/metaeuk_taxAnnot/"+ MetaEuk_taxAnnot
    megahit_version=megahit_version
    setn=setn
else:
    downloading_script="download_fastq_short.sh" # subsets only first 10k reads
    mincontiglen=100 #otherwise no contigs left, too few reads for 5000kb+ contigs
    EUK_PROFILES="/mnt/metaeuk_profiles_short/MMETSP_uniclust50_MERC_profiles_more_than1_few"
    EUK_taxAnnot="/mnt/metaeuk_taxAnnot/"+ MetaEuk_taxAnnot_small
    megahit_version="megahit_benchmark"
    setn=setn+"_bench"



###########################
## Megahit parallel mode?##
###########################

### Run several (2) megahits in parallel to occupy 100% CPU resources?
# Avoid when running big samples because RAM will be the limit and cause errors.
if (Megahit_parallel_mode==True):
    megahit_cores_coef=1/2
    megahit_memory_limit=0.65
else:
    megahit_cores_coef=1
    megahit_memory_limit=0.9

maxthreads = snakemake.utils.available_cpu_count()

print("Megahit_parallel_mode =", Megahit_parallel_mode)
###########################
##    Remote container   ##
###########################

# Attaching the remote S3 server for storing pipeline results.
# IMPORTANT: Global credential variables should be specified in bash environment 
#            before running current Snakefile, for example by sourcing a script.
#            Example script:
#              MinIO_secret.sh
#
#           export MINIO_HOST="https://openstack.cebitec.uni-bielefeld.de:8080"
#           export MINIO_ID="<secret-key-id>"
#           export MINIO_SECRET="<secret-key>"

import os
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host=os.environ["MINIO_HOST"],
                      access_key_id=os.environ["MINIO_ID"],
                      secret_access_key=os.environ["MINIO_SECRET"])

# de.nbi bielefeld container that stores a copy of metagenomes from ENA 
ena = S3RemoteProvider(host="https://openstack.cebitec.uni-bielefeld.de:8080",
                       access_key_id="",
                       secret_access_key="")



###########################
##  Email notifications  ##
###########################

targetmail="dembra96@gmail.com"

onstart:
    if (Quick_mode==False):
        print("Workflow started, sample set: ", setn)
        shell("mail -s 'Snakemake "+setn+" - Started' %s < {log}" % targetmail)
        shell("ssh ${{SLURM_SUBMIT_HOST}} 'mkdir -p ~/snakemake_stamps'")
        shell("mkdir -p ~/snakemake_stamps; touch ~/snakemake_stamps/%s_started_on_$(hostname)" % setn)
        shell("scp ~/snakemake_stamps/%s_started_on_$(hostname) ${{SLURM_SUBMIT_HOST}}:~/snakemake_stamps/" % setn)

onsuccess:
    if (Quick_mode==False):
        print("Workflow finished, no error")
        shell("mail -s 'Snakemake "+setn+" - Finished' %s < {log}" % targetmail)
        shell("touch ~/snakemake_stamps/%s_finished_on_$(hostname)" % setn)
        shell("scp ~/snakemake_stamps/%s_finished_on_$(hostname) ${{SLURM_SUBMIT_HOST}}:~/snakemake_stamps/" % setn)

onerror:
    if (Quick_mode==False):
        print("An error occurred")
        shell("mail -s 'Snakemake "+setn+" - ERROR' %s < {log}" % targetmail)
        shell("touch ~/snakemake_stamps/%s_ERROR_on_$(hostname)" % setn)
        shell("scp ~/snakemake_stamps/%s_ERROR_on_$(hostname) ${{SLURM_SUBMIT_HOST}}:~/snakemake_stamps/" % setn)










############################################
###                                      ###
###               Workflow               ###
###                                      ###
############################################


############################
##        Main rule       ##
############################

# Determines required output of the pipeline

rule All:
    input:
        MetaEuk = S3.remote(
           "eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/"+
                MetaEuk_version+"/"+MetaEuk_reference+"/"+setn+"_MetaEuk_preds.fasta.tar.gz"),

        MetaEuk_taxAnnot = S3.remote(
           "eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/"+
                MetaEuk_version+"/"+MetaEuk_reference+"/"+setn+"_MetaEuk_preds_taxAnnot.tar.gz"),

        sample_list = setn+"/"+setn+"_samples.txt",

        preprcoessing_reports = S3.remote(expand(
           "eukcontainer/contigs/"+megahit_version+"/preprocessing_reports/{sample}.reports.tar.gz",
                                                 sample=config["SAMPLE"])),

        QUASTs_raw_megahit_contigs = S3.remote(expand(
           "eukcontainer/contigs/"+megahit_version+"/QUAST_raw_contigs/{sample}.quast.raw.tar.gz",
                                                 sample=config["SAMPLE"])),

        QUASTs_sizefiltered_megahit_contigs = S3.remote(expand(
           "eukcontainer/contigs/"+megahit_version+"/QUAST_raw_contigs/{sample}.quast.sizefilt.tar.gz",
                                                 sample=config["SAMPLE"])),

        QUASTs_EukRep_contigs = S3.remote(expand(
           "eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+
                "/QUAST_EukRep/{sample}.quast.EukRep.tar.gz", sample=config["SAMPLE"])),

        MultiQC = S3.remote(
           "eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/"+
                MetaEuk_version+"/"+MetaEuk_reference+"/"+setn+"_MultiQC.tar.gz") 
    output:
        S3.remote("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/"+MetaEuk_version+"/"+MetaEuk_reference+"/"+setn+"_samples.txt"),
    shell:
        "cp {input.sample_list} {output[0]} ; "
        "touch "+setn+"/SET_PROCESSING_COMPLETELY_FINISHED ; "




############################
##      Service rules     ##
############################


#rule to save sample accessions provided in yaml as simple text list to copy to remote container.
rule Write_samples:
    output:
        setn+"/"+setn+"_samples.txt",
    threads: 1
    run:
        with open(output[0], 'w') as f:
            for item in config["SAMPLE"]:
                f.write("%s\n" % item)

    
### rule to download Protein profiles for MetaEuk if absent
# normally would be run only once with the very first snakemake launch on a new machine
rule GET_METAEUK_PROTEIN_PROFILES:
    output:
        "/mnt/metaeuk_profiles/metaeuk_ref_profiles_more_than1_MMETSP_zenodo_3247846_profiles"
    threads: 1
    shell:
        "if mc cp --recursive eukupload/eukcontainer/metaeuk_profiles /mnt/ ; "
        "then echo 'Protein profiles successfully copied from eukcontainer mc copy.'; "
        "else wget http://wwwuser.gwdg.de/~elevyka/metaeuk_ref_more_than1_MMETSP_zenodo_3247846_profiles.tar.gz; "
        "tar -C /mnt/metaeuk_profiles -xzf metaeuk_ref_more_than1_MMETSP_zenodo_3247846_profiles.tar.gz; "
        "rm metaeuk_ref_more_than1_MMETSP_zenodo_3247846_profiles.tar.gz; "
        "echo 'Protein profiles successfully downloaded from gwdg server.'; "
        "fi; "

rule GET_METAEUK_PROTEIN_PROFILES_short:
    output:
        "/mnt/metaeuk_profiles_short/MMETSP_uniclust50_MERC_profiles_more_than1_few"
    threads: 1
    shell:
        "if mc cp --recursive eukupload/eukcontainer/metaeuk_profiles_short /mnt/ ; "
        "then echo 'Protein profiles successfully copied from eukcontainer mc copy.'; "
        "else wget http://wwwuser.gwdg.de/~elevyka/MMETSP_uniclust50_MERC_profiles_more_than1_few.tar.gz; "
        "tar -C /mnt/metaeuk_profiles_short -xzf MMETSP_uniclust50_MERC_profiles_more_than1_few.tar.gz; "
        "rm MMETSP_uniclust50_MERC_profiles_more_than1_few.tar.gz; "
        "echo 'Protein profiles successfully downloaded from gwdg server.'; "
        "fi; "


### rule to download Taxonomic Annotation DB for MetaEuk if absent
# normally would be run only once with the very first snakemake launch on a new machine
rule GET_METAEUK_TAXANNOTDB:
    output:
        "/mnt/metaeuk_taxAnnot/"+ MetaEuk_taxAnnot
    threads: 1
    shell:
        "if mc cp --recursive eukupload/eukcontainer/metaeuk_taxAnnot /mnt/ ; "
        "then echo 'Protein Tax Annotation DB successfully copied from eukcontainer mc copy.'; "
        "else wget http://wwwuser.gwdg.de/~compbiol/metaeuk/2020_TAX_DB/"+MetaEuk_taxAnnot+".tar.gz; "
        "tar -C /mnt/metaeuk_taxAnnot/ -xzf "+MetaEuk_taxAnnot+".tar.gz; "
        "rm "+MetaEuk_taxAnnot+".tar.gz; "
        "echo 'Protein Tax Annotation DB successfully downloaded from gwdg server.'; "
        "fi; "

# rule to create a small sub-file of Taxonomic Annotation DB for MetaEuk for quick run
rule GET_METAEUK_TAXANNOTDB_SMALL:
    input:
        "/mnt/metaeuk_taxAnnot/"+ MetaEuk_taxAnnot
    output:
        "/mnt/metaeuk_taxAnnot/"+ MetaEuk_taxAnnot_small
    threads: 1
    shell: 
        "cd /mnt/metaeuk_taxAnnot/ ; "
        "metaeuk filtertaxseqdb "+MetaEuk_taxAnnot+" "+MetaEuk_taxAnnot_small+" --taxon-list 304574"



############################
##      Preprocessing     ##
############################


rule Download_fastq:
    output: 
        temp(setn+"/{sample}/reads_1.fastq.gz"),
        temp(setn+"/{sample}/reads_2.fastq.gz"),
    threads: 1
    shell: 
        snakemake_folder + "/scripts/" + downloading_script + " {wildcards.sample} {output}" 


rule FastQC_untrimmed:
    input:
        setn+"/{sample}/reads_1.fastq.gz",
        setn+"/{sample}/reads_2.fastq.gz"
    output:
        expand(setn+"/{{sample}}/reads_{n}_fastqc.{ext}", n=["1","2"], ext=["html","zip"]),
    conda: "yaml/conda_packages.yaml"
    threads: 2
    shell: 
        "fastqc -q -t {threads} {input} "

        
rule Fastp_trimming:
    input: 
        setn+"/{sample}/reads_1.fastq.gz",
        setn+"/{sample}/reads_2.fastq.gz"
    output: 
        temp(setn+"/{sample}/reads_1_trim.fastq.gz"),
        temp(setn+"/{sample}/reads_2_trim.fastq.gz"),
        setn+"/{sample}/fastp.json",
        setn+"/{sample}/fastp.html" 
    threads: 3
    conda: "yaml/conda_packages.yaml"
    shell: 
        "fastp -r -w 4 -i {input[0]} -I {input[1]} -o {output[0]} -O {output[1]} -j {output[2]} -h {output[3]} ; "

rule FastQC_trimmed:
    input:
        setn+"/{sample}/reads_1_trim.fastq.gz",
        setn+"/{sample}/reads_2_trim.fastq.gz"
    output:
        expand(setn+"/{{sample}}/reads_{n}_trim_fastqc.{ext}", n=["1","2"], ext=["html","zip"]),
    conda: "yaml/conda_packages.yaml"
    threads: 2
    shell:
        "fastqc -q -t {threads} {input} "


rule Upload_preprocessing_reports:
    input:
       fastqc1 = expand(setn+"/{{sample}}/reads_{n}_fastqc.{ext}", n=["1","2"], ext=["html","zip"]),
       fastp = [setn+"/{sample}/fastp.json", setn+"/{sample}/fastp.html"],
       fastqc2 = expand(setn+"/{{sample}}/reads_{n}_trim_fastqc.{ext}", n=["1","2"], ext=["html","zip"]),
    output:
       S3.remote("eukcontainer/contigs/"+megahit_version+"/preprocessing_reports/{sample}.reports.tar.gz")
    threads: 1
    shell:
        # some manipulations to avoid having files under "set_x" folder in tar file:
        "mkdir -p tmp_reports_{wildcards.sample}/{wildcards.sample} ; "
        "cp {input} tmp_reports_{wildcards.sample}/{wildcards.sample}/ ; "
        "tar -czf {output} -C tmp_reports_{wildcards.sample} $(ls tmp_reports_{wildcards.sample}/) ; "
        "rm -r tmp_reports_{wildcards.sample} ; "





############################
##        Assembly        ##
############################

rule Megahit_unfiltered:
    input:
        setn+"/{sample}/reads_1_trim.fastq.gz",
        setn+"/{sample}/reads_2_trim.fastq.gz"
    params:
        dir = setn+"/{sample}/megahit_unfiltered",
    output:
        contigs = setn+"/{sample}/contigs.megahit.fa",
    log:
        setn+"/{sample}/log_megahit.txt" #need to explicitly write in tar operation
    conda: "yaml/megahit.yaml"
    threads: int( maxthreads * megahit_cores_coef ) # Dependent on Megahit_parallel_mode option
    # This will allow two megahits run in parallell and make sure all cores are 100% busy all the time, 
    # because with single megahit run on all cores they are ~75% occupied on average.
    # However, if big files are processed, non-parallel mode is required to not run out of RAM.
    shell:
        # If there is an unfinished megahit - finish it. Otherwise run full megahit.
        "if [[ ! -d {params.dir} ]]; then " # normal megahit
            "echo 'Running normal megahit for {wildcards.sample}' ; "
            "{{ /usr/bin/time -v megahit -t "+str(maxthreads)+" -1 {input[0]} -2 {input[1]} -o {params.dir} --min-contig-len 500 --memory "+str(megahit_memory_limit)+"; }} 2> {log} ; "
        "else "
            "if [[ -d {params.dir} && ! -f {params.dir}/done ]]; then " # continue unfinished megahit.
                "echo -e '\\n\\n[[ATTENTION]]: Continuing unfinished megahit for {wildcards.sample}\\n\\n' ; "
                "{{ /usr/bin/time -v megahit -t "+str(maxthreads)+" -1 {input[0]} -2 {input[1]} -o {params.dir} --min-contig-len 500 --memory "+str(megahit_memory_limit)+" --continue; }} 2> {log} ; "
            "else " # delete old megahit and run a new one. It is a rerun. Otherwise this case wouldn't happen.
                "echo -e '\\n\\n[[ATTENTION]]: RE-running megahit for {wildcards.sample}\\n\\n' ; "
                "rm -r {params.dir} ; "
                "{{ /usr/bin/time -v megahit -t "+str(maxthreads)+" -1 {input[0]} -2 {input[1]} -o {params.dir} --min-contig-len 500 --memory "+str(megahit_memory_limit)+"; }} 2> {log} ; "
            "fi ; "
        "fi ; "
        "cp {params.dir}/final.contigs.fa {output.contigs} ; " # copy results one folder up for proper QUAST labeling later


rule Megahit_upload:
    input:
        setn+"/{sample}/contigs.megahit.fa", #{output[1]} #need to explicitly write in tar operation
        setn+"/{sample}/log_megahit.txt",
    params:
        megahit_intermaediate_contigs_dir = setn+"/{sample}/megahit_unfiltered/intermediate_contigs"
    output:
        S3.remote("eukcontainer/contigs/"+megahit_version+"/megahit/{sample}.contigs.megahit.fa.tar.gz"), #{output[2]}
    threads: 1
    shell:
        "touch {output} ; " # IMPORTANT! A stamp to reserve the temp local folder,
                            # so no finished tarring processes will delete it before all other tars also finish. 
        "tar -cf {output} -I 'pigz' -C "+setn+" {wildcards.sample}/contigs.megahit.fa {wildcards.sample}/log_megahit.txt ; "
        # Cleanup the huge intermediate contigs folder
        "rm -r {params.megahit_intermaediate_contigs_dir} ; "





############################
##    Contig processing   ##
############################


rule Raw_contigs_size_filtering:
    input:
        S3.remote("eukcontainer/contigs/"+megahit_version+"/megahit/{sample}.contigs.megahit.fa.tar.gz"),
    params:
        contigs = setn+"/{sample}/contigs.megahit.fa", # intermediate file
    output:
        setn+"/{sample}/contigs.megahit.sizefiltered.fa", # {output[0]}
    threads: 1
    shell:
        #unpack megahit contigs tar
        "mkdir -p {setn} ; "
        "tar -xf {input} -C {setn} ; " #will give intermediate file

        # Run sizefiltering
        'awk \'BEGIN {{FS = "\\t" ; OFS = "\\n"}} {{header = $0 ; getline seq ; '
        'if (length(seq) >= '+str(mincontiglen)+') {{print header, seq}}}}\' < {params.contigs} > {output[0]}; '
        ## ALWAYS escape { } \ by {{ }} \\ in snakemake, if you want to pass { } \ to a command, such as awk




rule EukRep:
    input:
        setn+"/{sample}/contigs.megahit.sizefiltered.fa"
    output:
        setn+"/{sample}/contigs.EukRep.fa",
        S3.remote("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/eukrep/{sample}.contigs.EukRep.fa.tar.gz"), #{output[1]}
    log:
        setn+"/{sample}/log_EukRep.txt"
    threads: 1
    shell:
        "touch {output[1]} ; "
        "{{ time EukRep -i {input} -o {output[0]} -m "+EukRep_mode+" --min "+str(mincontiglen)+"; }} 2> {log} ; "
        "tar -cf {output[1]} -I 'pigz' -C " + setn + 
        " {wildcards.sample}/contigs.EukRep.fa {wildcards.sample}/log_EukRep.txt "
        " {wildcards.sample}/contigs.megahit.sizefiltered.fa"





############################
##  QUAST contig analysis ##
############################


rule QUAST_raw_contigs:
    input:
        S3.remote("eukcontainer/contigs/"+megahit_version+"/megahit/{sample}.contigs.megahit.fa.tar.gz"),
        setn+"/{sample}/contigs.megahit.sizefiltered.fa", # to avoid quast and sizefilt accessing same file simultaneously
    params:
        contigs=setn+"/{sample}/contigs.megahit.fa", # intermediate file
        tmp="tmp_quast_raw_{sample}",                # temporary folder
	dir=setn+"/quasts/quast_raw_{sample}",       # results folder with log
    output:
        S3.remote("eukcontainer/contigs/"+megahit_version+"/QUAST_raw_contigs/{sample}.quast.raw.tar.gz"), # {output[0]}
    conda: "yaml/quast.yaml"
    threads: 1
    shell:
        #unpack megahit contigs tar
        "mkdir -p {setn} ; "
        "tar -xf {input[0]} -C {setn} ; " # will give intermediate file

        # Run QUAST
        # Conditional execution based on if contig file is empty or not.
        # If empty - run with avoiding error to not break the Snakemake, but still have the folder.
        "if [[ -s {params.contigs} ]] ; "
        "   then quast -t {threads} --silent -o {params.dir} {params.contigs} ; "
        "else "
        "   quast -t {threads} --silent -o {params.dir} {params.contigs} || "
        "      echo -e '\\n\\n[[[ATTENTION]]]: contig file {params.contigs} for QUAST was empty\\n\\n' ; "
        "fi ; "

        # Pack QUAST results
        # some manipulations to avoid having files under "set_x" folder in tar file:
        "mkdir -p {params.tmp}/quasts ; "
        "cp -r {params.dir} {params.tmp}/quasts/ ; "
        "tar -czf {output[0]} -C {params.tmp} $(ls {params.tmp}/) ; "
        "rm -r {params.tmp} ; "


rule QUAST_raw_contigs_sizefiltered:
    input:
        setn+"/{sample}/contigs.megahit.sizefiltered.fa",
    params:
        tmp="tmp_quast_sizefilt_{sample}",  # temporary folder
        dir=setn+"/quasts/quast_filtsize_{sample}",  # results folder with log
    output:
        S3.remote("eukcontainer/contigs/"+megahit_version+"/QUAST_raw_contigs/{sample}.quast.sizefilt.tar.gz"),
    conda: "yaml/quast.yaml"
    threads: 1
    shell:
        # Run QUAST
        # Conditional execution based on if contig file is empty or not.
        # If empty - run with avoiding error to not break the Snakemake, but still have the folder.
        "if [[ -s {input[0]} ]] ; "
        "   then   quast -t {threads} --silent --min-contig "+str(mincontiglen)+" -o {params.dir} {input[0]} ; "
        "else "
        "   quast -t {threads} --silent --min-contig "+str(mincontiglen)+" -o {params.dir} {input[0]} || "
        "      echo -e '\\n\\n[[[ATTENTION]]]: contig file {input[0]} for QUAST was empty\\n\\n' ; "
        "fi ; "

        # Pack QUAST results
        "mkdir -p {params.tmp}/quasts ; "
        "cp -r {params.dir} {params.tmp}/quasts/ ; "
        "tar -czf {output[0]} -C {params.tmp} $(ls {params.tmp}/) ; "
        "rm -r {params.tmp} ; "



rule QUAST_EukRep:
    input:
        S3.remote("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/eukrep/{sample}.contigs.EukRep.fa.tar.gz"),
    params:
        contigs=setn+"/{sample}/contigs.EukRep.fa", # intermediate file
        tmp="tmp_quast_eukrep_{sample}",            # temporary folder
        dir=setn+"/quasts/quast_EukRep_{sample}",   # results folder with log
    output:
        S3.remote("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/QUAST_EukRep/{sample}.quast.EukRep.tar.gz"),
    conda: "yaml/quast.yaml"
    threads: 1
    shell: 
        #unpack EukRep contigs tar
        "mkdir -p {setn} ; "
        "tar -xf {input} -C {setn} ; " # will give intermediate file

        # Run QUAST
        # Conditional execution based on if contig file is empty or not.
        # If empty - run with avoiding error to not break the Snakemake, but still have the folder.
        "if [[ -s {params.contigs} ]] ; "
        "   then   quast -t {threads} --silent --min-contig "+str(mincontiglen)+" -o {params.dir} {params.contigs} ; "
        "else "
        "   quast -t {threads} --silent --min-contig "+str(mincontiglen)+" -o {params.dir} {params.contigs} || "
        "      echo -e '\\n\\n[[[ATTENTION]]]: contig file {params.contigs} for QUAST was empty\\n\\n' ; "
        "fi ; "    

        # Pack QUAST results
        "mkdir -p {params.tmp}/quasts ; "
        "cp -r {params.dir} {params.tmp}/quasts/ ; "
        "tar -czf {output[0]} -C {params.tmp} $(ls {params.tmp}/) ; "
        "rm -r {params.tmp} ; "





############################
##  Final MultiQC report  ##
############################


rule MultiQC:
    input:
        report_tars = S3.remote(expand("eukcontainer/contigs/"+megahit_version+"/preprocessing_reports/{sample}.reports.tar.gz", sample=config["SAMPLE"])),
        quasts_raw = S3.remote(expand("eukcontainer/contigs/"+megahit_version+"/QUAST_raw_contigs/{sample}.quast.raw.tar.gz", sample=config["SAMPLE"])),
        quasts_sizefilt = S3.remote(expand("eukcontainer/contigs/"+megahit_version+"/QUAST_raw_contigs/{sample}.quast.sizefilt.tar.gz", sample=config["SAMPLE"])),
        quasts_eukrep = S3.remote(expand("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/QUAST_EukRep/{sample}.quast.EukRep.tar.gz", sample=config["SAMPLE"])),
    params:
        tmp = "tmp_MultiQC_"+setn
    output:
        report = setn+"/multiqc_report.html",
        data = directory(setn+"/multiqc_data"),
        tar = S3.remote("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/"+MetaEuk_version+"/"+MetaEuk_reference+"/"+setn+"_MultiQC.tar.gz"),
    conda: "yaml/conda_packages.yaml"
    threads: 1
    shell:
        # Unpack the downloaded report archives
        "for i in {input.report_tars}; do  tar -xzf $i -C {setn}; done ; "
        "for i in {input.quasts_raw}; do  tar -xzf $i -C {setn}; done ; "
        "for i in {input.quasts_sizefilt}; do  tar -xzf $i -C {setn}; done ; "
        "for i in {input.quasts_eukrep}; do  tar -xzf $i -C {setn}; done ; "

        # Run MultiQC
        "multiqc -d -f -o "+setn+" {setn}/quasts/ {setn}/*RR*/*fastqc* {setn}/*RR*/*fastp* ; "
# -d option makes multiqc analyse all samples with same names. In our case all samples are reads_1/2, so this is crucial
# -f option makes force overwrite of previous multiqc output. Important for Snakemake reruns.
# -o specifies outdir of output files

        # Pack MultiQC report
        "mkdir -p {params.tmp} ; "
        "cp -r {output.report} {output.data} {params.tmp}/ ; "
        "tar -czf {output.tar} -C {params.tmp} $(ls {params.tmp}/) ; "
        "rm -r {params.tmp} ; "





############################
##         Metaeuk        ##
############################


# Concatenating all EukRep contigs to speedup Metaeuk by running it on a combined input
rule Contig_concatenating:
    input:
        S3.remote(expand("eukcontainer/contigs/{megahit_version}/{EukRep_version}_{EukRep_mode}/eukrep/{sample}.contigs.EukRep.fa.tar.gz",
                         sample=config["SAMPLE"],
                         setn=setn,
                         megahit_version=megahit_version,
                         EukRep_version=EukRep_version,
                         EukRep_mode=EukRep_mode))
    output:
        setn+"/EukRep_contigs_renamed_cat.fasta"
    threads: 1
    shell:
        "mkdir -p {setn} ;"
        "cat {input} | tar -xzivf - -C "+setn+ " ; " +
        snakemake_folder + "/scripts/contig_rename_concat.sh {output} " + 
        ' '.join(expand("{setn}/{sample}/contigs.EukRep.fa", setn=setn, sample=config['SAMPLE']))


# MetaEuk 
rule MetaEuk_easy_predict:
    input:
        contigs = setn+"/EukRep_contigs_renamed_cat.fasta",
        profiles = EUK_PROFILES,
        sample_list = setn+"/"+setn+"_samples.txt",
    params:
        tmp = "tmp_MetaEuk_"+setn,
        aux_files_folder = setn+"/tempFolder/latest",
        aux_files_subfolder = "tempFolder/latest",
        aux_files_pattern = "{contigs*,MetaEuk_preds.*}",
    log: setn+"/log_MetaEuk_easypredict.txt"
    output:
        fasta = setn+"/MetaEuk_preds.fasta",
        tempfolder = temp(directory(setn+"/tempFolder")),
        tsv = setn+"/MetaEuk_preds.fasta.headersMap.tsv",
        tar = S3.remote("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/"+MetaEuk_version+"/"+MetaEuk_reference+"/"+setn+"_MetaEuk_preds.fasta.tar.gz"),
    shell:
        #MetaEuk run
        "{{ time metaeuk easy-predict {input.contigs} {input.profiles} {output.fasta} {output.tempfolder}"
        " --metaeuk-eval 0.0001 -e 100 --slice-search --min-ungapped-score 35"
        " --min-length 40 --min-exon-aa 20 --metaeuk-tcov 0.6; }} > {log} 2>&1 ; "

        # Pack MetaEuk results
        "mkdir -p {params.tmp}/{params.aux_files_subfolder} ; "
        "cp {params.aux_files_folder}/{params.aux_files_pattern} {params.tmp}/{params.aux_files_subfolder}/ ; "
        "cp {output.fasta} {output.tsv} {log} {input.sample_list}   {params.tmp}/ ; "
        "tar -czf {output.tar} -C {params.tmp} $(ls {params.tmp}/) ; "
        "rm -r {params.tmp} ; "


# A variable to be used in the next rule:
metaeuk_out_base = setn+"/"+setn

rule MetaEuk_taxonomic_classification:
    input:
        Metaeuk_results  = S3.remote("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/"+MetaEuk_version+"/"+MetaEuk_reference+"/"+setn+"_MetaEuk_preds.fasta.tar.gz"),
        taxAnnotTargetDb = EUK_taxAnnot,
    params:
        contigsDB = setn+"/tempFolder/latest/contigs",
        preds_fasta = setn+"/MetaEuk_preds.fasta",
        preds_tsv   = setn+"/MetaEuk_preds.fasta.headersMap.tsv",
        sample_list = setn+"/"+setn+"_samples.txt",
        tmp = "tmp_MetaEuk_taxannot_"+setn,
        metaeuk_out_base = metaeuk_out_base,
        tax_per_pred = metaeuk_out_base+"_tax_per_pred.tsv",
        tax_per_contig = metaeuk_out_base+"_tax_per_contig.tsv",
    output:
        tempfolder_2 = temp(directory(setn+"/tempFolder_2")),
        tar = S3.remote("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/"+MetaEuk_version+"/"+MetaEuk_reference+"/"+setn+"_MetaEuk_preds_taxAnnot.tar.gz"),
    log: setn+"/log_MetaEuk_taxtocontig.txt"
    shell:
        # Unpack MetaEuk results:
        "mkdir -p {setn} ; "
        "tar -xf {input.Metaeuk_results} -C {setn} ; " 
        # Run MetaEuk tax annotator:
        "{{ time metaeuk taxtocontig {params.contigsDB} {params.preds_fasta} {params.preds_tsv} {input.taxAnnotTargetDb} "
          " {params.metaeuk_out_base} {output.tempfolder_2} --majority 0.5 --tax-lineage --lca-mode 2 && echo 'MetaEuk taxtocontig finished good, without error.' ; }} > {log} 2>&1 && "
        # Pack MetaEuk tax results:
        "mkdir -p {params.tmp} && "
        "echo 'copying' >> {log} && "
        "cp {params.tax_per_pred} {params.tax_per_contig} {log} {params.sample_list} {params.tmp}/ && "
        "tar -czf {output.tar} -C {params.tmp} $(ls {params.tmp}/) && "
        "rm -r {params.tmp} ; "

