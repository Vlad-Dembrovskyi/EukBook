# This is the pipeline to run MetaEuk from beginning (given sample accession) without any kraken filtering and bowtie mapping

# Compared to Snakefile_simple here we wisely concatenate all EukRep contigs and running Metaeuk on all of them together. Should speed up.

# IMPORTANT: ~/EukBook/MinIO_secret.sh must be sourced before Snakemake execution!

## Required config parameters:
# SAMPLE - passed with config file by --configfile samples.yaml or --config SAMPLE=SRR2844600
# SAMPLESET - for creating a subfolder dedicated to certain group of samples, listed in yaml file
# The batch number. It is the subfolder in our remote MinIO container,
# where our results are saved. A new batch makes new calculations for samples,
# that were already calculated for earlier batch. This is made to treat new versions
# of pipeline separately and don`t override old results.
BATCH="batch2" 

## Benchmarking mode?

#benchmark=True
benchmark=False

if (benchmark==False):
    downloading_script="download_fastq.sh"
    mincontiglen=5000
    EUK_PROFILES="/mnt/metaeuk_profiles/metaeuk_ref_profiles_more_than1_MMETSP_zenodo_3247846_profiles"
    BATCH=BATCH
else:
    downloading_script="download_fastq_short.sh" # subsets only first 10k reads
    mincontiglen=100 #otherwise no contigs left, too few reads for 5000kb+ contigs
    EUK_PROFILES="/mnt/metaeuk_profiles_short/MMETSP_uniclust50_MERC_profiles_more_than1_few"
    BATCH="batch_benchmark"

# hardcoded parameters:
mainfolder='/home/ubuntu/EukBook'
maxthreads = snakemake.utils.available_cpu_count()


# Attaching the remote MinIO S3 storage server
# to make snakemake access remote files
# IMPORTANT: ~/EukBook/MinIO_secret.sh must be sourced before Snakemake execution!
import os
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host=os.environ["MINIO_HOST"], access_key_id=os.environ["MINIO_ID"], secret_access_key=os.environ["MINIO_SECRET"])

ena = S3RemoteProvider(host="https://openstack.cebitec.uni-bielefeld.de:8080", access_key_id="", secret_access_key="")


# Handling different input ways of SAMPLESET config.
# SAMPLESET should be passed with yaml config file alongside with sample accessions.
# Then it is an array of values, of which we have to take only the first argument  - [0].
# But it can be overwritten with --config SAMPLESET=xxx parameter of snakemake command, if needed.
# Then it is a string itself, not array. Applying [0] to string would return only its first character.
if type(config["SAMPLESET"]) is list:
    setn=config["SAMPLESET"][0]
elif type(config["SAMPLESET"]) is str:
    setn=config["SAMPLESET"]



# Email notifications
onstart:
    if (benchmark==False):
        print("Workflow finished, no error")
        shell("mail -s 'Snakemake "+setn+" - Workflow started' dembra96@gmail.com < {log}" )

onsuccess:
    if (benchmark==False):
        print("Workflow finished, no error")
        shell("mail -s 'Snakemake "+setn+" - Workflow successfully finished' dembra96@gmail.com < {log}")

onerror:
    if (benchmark==False):
        print("An error occurred")
        shell("mail -s 'Snakemake "+setn+" - an error occurred' dembra96@gmail.com < {log}")




################
### Workflow ###
################


rule all:
    input:
        report = S3.remote("eukcontainer/"+BATCH+"/"+ setn+".reports.tar.gz"),
        main_archive = S3.remote("eukcontainer/"+BATCH+"/"+ setn+".tar.gz"),
        sample_list = S3.remote("eukcontainer/"+BATCH+"/"+ setn+"_samples.txt"),

#compress all needed files
rule tar_big_archive:
    input: 
        ##REPORTS
        reports = [
        #FastQC
        expand(setn+"/{sample}/reads_{n}{trim}_fastqc.{ext}", 
               sample=config["SAMPLE"], n=["1","2"], trim=["","_trim"], ext=["html","zip"]),
        #Fastp
        expand(setn+"/{sample}/fastp.html", sample=config["SAMPLE"]),
        expand(setn+"/{sample}/fastp.json", sample=config["SAMPLE"]),
        #QUAST
        setn+"/quast_multi_raw",
        setn+"/quast_multi_filt_raw",
        setn+"/quast_multi_EukRep", 
        #MultiQC
        setn+"/multiqc_report.html",
        setn+"/multiqc_data",
        #logs
        setn+"/log_MetaEuk_easypredict.txt",
        expand(setn+"/{sample}/log_megahit.txt", sample=config["SAMPLE"]),
        expand(setn+"/{sample}/log_EukRep.txt", sample=config["SAMPLE"])],
        
        ## RESULTS
        #Megahit
        megahit = expand(setn+"/{sample}/contigs.megahit.fa", sample=config["SAMPLE"]),
        #EukRep
        eukrep = expand(setn+"/{sample}/contigs.EukRep.fa", sample=config["SAMPLE"]),
        #MetaEuk
	metaeuk = [setn+"/MetaEuk_preds.fasta",
                   setn+"/MetaEuk_preds.fasta.headersMap.tsv"],
    output:
        main = S3.remote("eukcontainer/"+BATCH+"/"+ setn+".tar.gz"),
    shell:
        "tar -cf {output.main} -I 'pigz -p 14' {input} {setn}/tempFolder/latest/{{contigs*,MetaEuk_preds.*}}"

rule tar_reports:
    input:
        ##REPORTS
        reports = [
        #FastQC
        expand(setn+"/{sample}/reads_{n}{trim}_fastqc.{ext}",
               sample=config["SAMPLE"], n=["1","2"], trim=["","_trim"], ext=["html","zip"]),
        #Fastp
        expand(setn+"/{sample}/fastp.html", sample=config["SAMPLE"]),
        expand(setn+"/{sample}/fastp.json", sample=config["SAMPLE"]),
        #QUAST
        setn+"/quast_multi_raw",
        setn+"/quast_multi_filt_raw",
        setn+"/quast_multi_EukRep",
        #MultiQC
        setn+"/multiqc_report.html",
        setn+"/multiqc_data",
        #logs
        setn+"/log_MetaEuk_easypredict.txt",
        expand(setn+"/{sample}/log_megahit.txt", sample=config["SAMPLE"]),
        expand(setn+"/{sample}/log_EukRep.txt", sample=config["SAMPLE"])],
    output:
        report = S3.remote("eukcontainer/"+BATCH+"/"+ setn+".reports.tar.gz"),
    shell:
        "tar -czf {output.report} {input.reports}"


#rule to save sample accessions provided in yaml as simple text list to copy to remote container.
rule write_samples:
    output:
        sample_list = S3.remote("eukcontainer/"+BATCH+"/"+ setn+"_samples.txt"),
    run:
        with open(output[0], 'w') as f:
            for item in config["SAMPLE"]:
                f.write("%s\n" % item)
    
### rule to download Protein profiles for MetaEuk if absent
# normally would be run only once with the very first snakemake launch on a new machine
rule GET_METAEUK_PROTEIN_PROFILES:
    output:
        "/mnt/metaeuk_profiles/metaeuk_ref_profiles_more_than1_MMETSP_zenodo_3247846_profiles"
    shell:
        "if mc cp --recursive eukupload/eukcontainer/metaeuk_profiles /mnt/ ; "
        "then echo 'Protein profiles successfully copied from eukcontainer mc copy.'; "
        "else wget http://wwwuser.gwdg.de/~elevyka/metaeuk_ref_more_than1_MMETSP_zenodo_3247846_profiles.tar.gz; "
        "tar -C /mnt/metaeuk_profiles -xzf metaeuk_ref_more_than1_MMETSP_zenodo_3247846_profiles.tar.gz; "
        "rm metaeuk_ref_more_than1_MMETSP_zenodo_3247846_profiles.tar.gz; "
        "echo 'Protein profiles successfully downloaded from gwdg server.'; "
        "fi; "

rule GET_METAEUK_PROTEIN_PROFILES_short:
    output:
        "/mnt/metaeuk_profiles_short/MMETSP_uniclust50_MERC_profiles_more_than1_few"
    shell:
        "if mc cp --recursive eukupload/eukcontainer/metaeuk_profiles_short /mnt/ ; "
        "then echo 'Protein profiles successfully copied from eukcontainer mc copy.'; "
        "else wget http://wwwuser.gwdg.de/~elevyka/MMETSP_uniclust50_MERC_profiles_more_than1_few.tar.gz; "
        "tar -C /mnt/metaeuk_profiles_short -xzf MMETSP_uniclust50_MERC_profiles_more_than1_few.tar.gz; "
        "rm MMETSP_uniclust50_MERC_profiles_more_than1_few.tar.gz; "
        "echo 'Protein profiles successfully downloaded from gwdg server.'; "
        "fi; "



### Preprocessing
rule download_fastq:
    output: 
        setn+"/{sample}/reads_1.fastq.gz",
        setn+"/{sample}/reads_2.fastq.gz"
    shell: 
        "{mainfolder}/scripts/" + downloading_script + " {wildcards.sample} {output}" 


rule FastQC_untrimmed:
    input:
        setn+"/{sample}/reads_1.fastq.gz",
        setn+"/{sample}/reads_2.fastq.gz"
    output:
        expand(setn+"/{{sample}}/reads_{n}_fastqc.{ext}", n=["1","2"], ext=["html","zip"]),
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    threads: 2
    shell: 
        "fastqc -t {threads} {input} "

        
rule fastp_trimming:
    input: 
        setn+"/{sample}/reads_1.fastq.gz",
        setn+"/{sample}/reads_2.fastq.gz"
    output: 
        setn+"/{sample}/reads_1_trim.fastq.gz",
        setn+"/{sample}/reads_2_trim.fastq.gz",
        setn+"/{sample}/fastp.json",
        setn+"/{sample}/fastp.html" 
    threads: 4  
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    shell: 
        "fastp -r -w {threads} -i {input[0]} -I {input[1]} -o {output[0]} -O {output[1]} -j {output[2]} -h {output[3]}"


rule FastQC_trimmed:
    input:
        setn+"/{sample}/reads_1_trim.fastq.gz",
        setn+"/{sample}/reads_2_trim.fastq.gz"
    output:
        expand(setn+"/{{sample}}/reads_{n}_trim_fastqc.{ext}", n=["1","2"], ext=["html","zip"]),
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    threads: 2
    shell:
        "fastqc -t {threads} {input} "




### Assembly without filtering

rule megahit_unfiltered:
    input:
        setn+"/{sample}/reads_1_trim.fastq.gz",
        setn+"/{sample}/reads_2_trim.fastq.gz"
    output:
        directory(setn+"/{sample}/megahit_unfiltered"), #{output[0]}
        setn+"/{sample}/contigs.megahit.fa", #{output[1]}
        S3.remote("eukcontainer/megahit/{sample}.contigs.megahit.fa.tar.gz"), #{output[2]}
         
    log:
        setn+"/{sample}/log_megahit.txt"
    conda:
        mainfolder + "/yaml/megahit.yaml"
    threads: 7 
#this will allow two megahits run in parallell and make sure all cores are 100% busy al the time, because with single megahit on all cores they are ~75% occupied on average.
    shell:
        #"rm -r "+setn+"/{wildcards.sample}/megahit_unfiltered; "
        "touch {output[2]} ;" # IMPORTANT! A stamp to reserve the temp local folder so no finished meghit processes wil delete it before all others also finish. 
        "{{ time megahit -t 14 -1 {input[0]} -2 {input[1]} -o {output[0]} --min-contig-len 500; }} 2> {log}; "
        "cp {output[0]}/final.contigs.fa {output[1]}; " # copy results one folder up for proper QUAST labeling later
        "tar -cf {output[2]} -I 'pigz' {output[1]} {log}; "
        "touch {setn}/{wildcards.sample}/checkpoint_{wildcards.sample}_is_done.txt" 

rule raw_contigs_size_filtering:
    input:
        setn+"/{sample}/contigs.megahit.fa"
    output:
        setn+"/{sample}/contigs.megahit.sizefiltered.fa"
    shell:
        #for fasta:
        'awk \'BEGIN {{FS = "\\t" ; OFS = "\\n"}} {{header = $0 ; getline seq ; if (length(seq) >= '+str(mincontiglen)+') {{print header, seq}}}}\' < {input} > {output}; '
        #for fastq (but I have fasta):
        #'awk \'BEGIN {{FS = "\\t" ; OFS = "\\n"}} {{header = $0 ; getline seq ; getline qheader ; getline qseq ; if (length(seq) >= '+str(mincontiglen)+') {{print header, seq, qheader, qseq}}}}\' < {input} > {output}; '
# ALWAYS escape { } \ by {{ }} \\ in snakemake, if you want to pass { } \ to a command, such as awk 


rule EukRep:
    input:
        setn+"/{sample}/contigs.megahit.sizefiltered.fa"
    output:
        setn+"/{sample}/contigs.EukRep.fa"
    log:
        setn+"/{sample}/log_EukRep.txt"
    shell:
        "{{ time EukRep -i {input} -o {output} -m lenient --min "+str(mincontiglen)+"; }} 2> {log} ; "

# QUAST

rule QUAST_multisample_raw_contigs:
    input:
        expand("{setn}/{sample}/contigs.megahit.fa", sample=config["SAMPLE"],setn=setn)
    output:
        directory(setn+"/quast_multi_raw")
    conda:
        mainfolder + "/yaml/quast.yaml"
    threads: 4
    shell:
        #"time quast -t {threads} --silent -o {output} {input}"
        "quast -t {threads} --silent --min-contig "+str(mincontiglen)+" -o {output} {input}"

rule QUAST_multisample_raw_contigs_sizefiltered:
    input:
        expand("{setn}/{sample}/contigs.megahit.sizefiltered.fa", sample=config["SAMPLE"],setn=setn)
    output:
        directory(setn+"/quast_multi_filt_raw")
    conda:
        mainfolder + "/yaml/quast.yaml"
    threads: 4
    shell:
        "quast -t {threads} --silent --min-contig "+str(mincontiglen)+" -o {output} {input}; "

rule QUAST_multisample_EukRep:
    input:
        expand("{setn}/{sample}/contigs.EukRep.fa", sample=config["SAMPLE"],setn=setn)
    output:
        directory(setn+"/quast_multi_EukRep")
    conda:
        mainfolder + "/yaml/quast.yaml"
    threads: 4
    shell: 
        #"time quast -t {threads} --silent -o {output} {input}"
        "quast -t {threads} --silent --min-contig "+str(mincontiglen)+" -o {output} {input}"



# Concatenating all EukRep contigs
rule contig_concatenating:
    input:
        expand("{setn}/{sample}/contigs.EukRep.fa", sample=config["SAMPLE"],setn=setn)
    output:
        setn+"/EukRep_contigs_renamed_cat.fasta"
    shell:
        mainfolder + "/scripts/contig_rename_concat.sh {output} {input}" 


# MetaEuk 
rule MetaEuk_easy_predict:
    input:
        setn+"/EukRep_contigs_renamed_cat.fasta", #{input[0]}
        EUK_PROFILES        #{input[1]}
    output:
        setn+"/MetaEuk_preds.fasta",        #{output[0]}
        directory(setn+"/tempFolder"), #{output[1]}
        setn+"/MetaEuk_preds.fasta.headersMap.tsv", #just need to indicate it here so tar ruleknows where to find it. no need to specify it in metaeuk command.
    log:
        setn+"/log_MetaEuk_easypredict.txt"
    threads: maxthreads
    resources: mem_mb=120000
    shell:
        "{{ time metaeuk easy-predict {input[0]} {input[1]} {output[0]} {output[1]}"
        " --metaeuk-eval 0.0001 -e 100 --slice-search --min-ungapped-score 35"
        " --min-length 40 --min-exon-aa 20 --metaeuk-tcov 0.6; }} > {log} 2>&1"


# MultiQC
rule MultiQC:
    input:
        expand(setn+"/{sample}/{file}", sample=config["SAMPLE"], file=['reads_1_fastqc.html', 'reads_2_fastqc.html', 'reads_1_trim_fastqc.html', 'reads_2_trim_fastqc.html', 'fastp.html']),
        expand(setn+"/{file}", file=['quast_multi_raw', 'quast_multi_EukRep', 'quast_multi_filt_raw'])
    output:
        setn+"/multiqc_report.html",
        directory(setn+"/multiqc_data")
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    shell:
        "multiqc -d -f -o "+setn+" "+setn+"/"
# -d option makes multiqc analyse all samples with same names. In our case all samples are reads_1/2, so this is crucial
# -f option makes force overwrite of previous multiqc output. Important for Snakemake reruns.
# -o specifies outdir of output files
