# This is the pipeline to run MetaEuk from beginning (given sample accession) without any kraken filtering and bowtie mapping

# Compared to Snakefile_simple here we wisely concatenate all EukRep contigs and running Metaeuk on all of them together. Should speed up.

# IMPORTANT: ~/EukBook/MinIO_secret.sh must be sourced before Snakemake execution!






############################################
###                                      ###
###             Configuration            ###
###                                      ###
############################################

# Main arguments to the Megahit Snakemake pipeline must be passed with a config.

#### Required input config parameters: ####
# (must be passed with --configafile from yaml or json file, or in Snakemake command with --config)
# (accessed in current snakefile with config['PARAMETER'])
#
#  * SAMPLE    - list of desired sample accessions to obtain MetaEuk predictions for.
#  * SAMPLESET - unique name for the set of selected accesions. 
#                Will be used to create a local subfolder and remote MetaEuk results folder.

###  Example config usage
#   snakemake [other arguments] --config SAMPLE=SRR2844600 SAMPLESET=set_07 
#   snakemake [other arguments] --configfile path/to/configfile.yaml
#       Example configfile.yaml:
#            SAMPLESET:
#               - set1
#            SAMPLE:
#               - SRR5918406
#               - SRR5788362
#               - SRR5788367
#               - ...
#
#       NOTE: --config can pass only one string value for a parameter, e.g. only one sample.
#             Passing multiple samples is possible only with config file. 
#       NOTE: If both --config and --configfile are used and specify parameter with same name, 
#             --config parameter will overwrite the one passed from --configfile.
#             Parameters with different names will end up in final config independently 
#             of way they were specified.







############################
##  Hardcoded parameters  ##
############################

mainfolder='/home/ubuntu/EukBook'
maxthreads = snakemake.utils.available_cpu_count()
megahit_version = "megahit_v1.2.9" # Snakemake will create this remote folder for megahit results.
                                   # Prevents overwriting if different version of assembler is used.
EukRep_version = "EukRep_v0.6.6"
EukRep_mode = "lenient"
MetaEuk_version = "MetaEuk_alpha"
MetaEuk_reference = "metaeuk_ref_profiles_more_than1_MMETSP_zenodo_3247846_profiles"




## Handling different input ways of SAMPLESET config. ##
# Normally SAMPLESET should be passed with configfile together with corresponding accessions.
#    In this case yaml always passes an array, even if only with one value.
#    So, to obtain the actual sampleset string value, need to take first element of such array input.
# But if the sampleset is explicitly specified with --config SAMPLESET="set_somethin",
#   then it is already a string and if we are taking first element we will get only the first character.
if type(config["SAMPLESET"]) is list:
    setn=config["SAMPLESET"][0]
elif type(config["SAMPLESET"]) is str:
    setn=config["SAMPLESET"]



###########################
##      Quick mode?      ##
###########################
#Quick=True
Quick=False


#### Quick mode: Changing Snakemake behavior to quickly run through using micro-subsetted files ####
if (Quick==False):
    downloading_script="download_fastq.sh"
    mincontiglen=5000
    EUK_PROFILES="/mnt/metaeuk_profiles/"+ MetaEuk_reference
    megahit_version=megahit_version
    setn=setn
else:
    downloading_script="download_fastq_short.sh" # subsets only first 10k reads
    mincontiglen=100 #otherwise no contigs left, too few reads for 5000kb+ contigs
    EUK_PROFILES="/mnt/metaeuk_profiles_short/MMETSP_uniclust50_MERC_profiles_more_than1_few"
    megahit_version="megahit_benchmark"
    setn=setn+"_bench"



###########################
##    Remote container   ##
###########################

# Attaching the remote S3 server for storing pipeline results.
# IMPORTANT: Global credential variables should be specified in bash environment 
#            before running current Snakefile, for example by sourcing a script.
#            Example script:
#              MinIO_secret.sh
#
#           export MINIO_HOST="https://openstack.cebitec.uni-bielefeld.de:8080"
#           export MINIO_ID="<secret-key-id>"
#           export MINIO_SECRET="<secret-key>"

import os
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host=os.environ["MINIO_HOST"],
                      access_key_id=os.environ["MINIO_ID"],
                      secret_access_key=os.environ["MINIO_SECRET"])

# de.nbi bielefeld container that stores a copy of metagenomes from ENA 
ena = S3RemoteProvider(host="https://openstack.cebitec.uni-bielefeld.de:8080",
                       access_key_id="",
                       secret_access_key="")






###########################
##  Email notifications  ##
###########################

targetmail="dembra96@gmail.com"

onstart:
    if (Quick==False):
        print("Workflow finished, no error")
        shell("mail -s 'Snakemake "+setn+" - Workflow started' %s < {log}" % targetmail)

onsuccess:
    if (Quick==False):
        print("Workflow finished, no error")
        shell("mail -s 'Snakemake "+setn+" - Workflow successfully finished' %s < {log}" % targetmail)

onerror:
    if (Quick==False):
        print("An error occurred")
        shell("mail -s 'Snakemake "+setn+" - an error occurred' %s < {log}" % targetmail)











############################################
###                                      ###
###               Workflow               ###
###                                      ###
############################################


############################
##        Main rule       ##
############################

# Determines required output of the pipeline

rule all:
    input:
        MetaEuk = S3.remote("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/"+MetaEuk_version+"/"+MetaEuk_reference+"/"+setn+"_MetaEuk_preds.fasta.tar.gz"),
        sample_list = S3.remote("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/"+MetaEuk_version+"/"+MetaEuk_reference+"/"+setn+"_samples.txt"),
        preprcoessing_reports = S3.remote(expand("eukcontainer/contigs/"+megahit_version+"/preprocessing_reports/{sample}.reports.tar.gz", sample=config["SAMPLE"])),
        QUASTs_raw_megahit_contigs = S3.remote(expand("eukcontainer/contigs/"+megahit_version+"/QUAST_raw_contigs/{sample}.quast.raw.tar.gz", sample=config["SAMPLE"])),
        MultiQC = setn+"/multiqc_report.html",







############################
##      Service rules     ##
############################


#rule to save sample accessions provided in yaml as simple text list to copy to remote container.
rule write_samples:
    output:
        S3.remote("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/"+MetaEuk_version+"/"+MetaEuk_reference+"/"+setn+"_samples.txt"),
    run:
        with open(output[0], 'w') as f:
            for item in config["SAMPLE"]:
                f.write("%s\n" % item)

    
### rule to download Protein profiles for MetaEuk if absent
# normally would be run only once with the very first snakemake launch on a new machine
rule GET_METAEUK_PROTEIN_PROFILES:
    output:
        "/mnt/metaeuk_profiles/metaeuk_ref_profiles_more_than1_MMETSP_zenodo_3247846_profiles"
    shell:
        "if mc cp --recursive eukupload/eukcontainer/metaeuk_profiles /mnt/ ; "
        "then echo 'Protein profiles successfully copied from eukcontainer mc copy.'; "
        "else wget http://wwwuser.gwdg.de/~elevyka/metaeuk_ref_more_than1_MMETSP_zenodo_3247846_profiles.tar.gz; "
        "tar -C /mnt/metaeuk_profiles -xzf metaeuk_ref_more_than1_MMETSP_zenodo_3247846_profiles.tar.gz; "
        "rm metaeuk_ref_more_than1_MMETSP_zenodo_3247846_profiles.tar.gz; "
        "echo 'Protein profiles successfully downloaded from gwdg server.'; "
        "fi; "

rule GET_METAEUK_PROTEIN_PROFILES_short:
    output:
        "/mnt/metaeuk_profiles_short/MMETSP_uniclust50_MERC_profiles_more_than1_few"
    shell:
        "if mc cp --recursive eukupload/eukcontainer/metaeuk_profiles_short /mnt/ ; "
        "then echo 'Protein profiles successfully copied from eukcontainer mc copy.'; "
        "else wget http://wwwuser.gwdg.de/~elevyka/MMETSP_uniclust50_MERC_profiles_more_than1_few.tar.gz; "
        "tar -C /mnt/metaeuk_profiles_short -xzf MMETSP_uniclust50_MERC_profiles_more_than1_few.tar.gz; "
        "rm MMETSP_uniclust50_MERC_profiles_more_than1_few.tar.gz; "
        "echo 'Protein profiles successfully downloaded from gwdg server.'; "
        "fi; "







############################
##      Preprocessing     ##
############################


rule download_fastq:
    output: 
        setn+"/{sample}/reads_1.fastq.gz",
        setn+"/{sample}/reads_2.fastq.gz"
    shell: 
        "{mainfolder}/scripts/" + downloading_script + " {wildcards.sample} {output}" 


rule FastQC_untrimmed:
    input:
        setn+"/{sample}/reads_1.fastq.gz",
        setn+"/{sample}/reads_2.fastq.gz"
    output:
        expand(setn+"/{{sample}}/reads_{n}_fastqc.{ext}", n=["1","2"], ext=["html","zip"]),
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    threads: 2
    shell: 
        "fastqc -t {threads} {input} "

        
rule fastp_trimming:
    input: 
        setn+"/{sample}/reads_1.fastq.gz",
        setn+"/{sample}/reads_2.fastq.gz"
    output: 
        setn+"/{sample}/reads_1_trim.fastq.gz",
        setn+"/{sample}/reads_2_trim.fastq.gz",
        setn+"/{sample}/fastp.json",
        setn+"/{sample}/fastp.html" 
    threads: 4  
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    shell: 
        "fastp -r -w {threads} -i {input[0]} -I {input[1]} -o {output[0]} -O {output[1]} -j {output[2]} -h {output[3]}"


rule FastQC_trimmed:
    input:
        setn+"/{sample}/reads_1_trim.fastq.gz",
        setn+"/{sample}/reads_2_trim.fastq.gz"
    output:
        expand(setn+"/{{sample}}/reads_{n}_trim_fastqc.{ext}", n=["1","2"], ext=["html","zip"]),
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    threads: 2
    shell:
        "fastqc -t {threads} {input} "


rule upload_preprocessing_reports:
    input:
       fastqc1 = expand(setn+"/{{sample}}/reads_{n}_fastqc.{ext}", n=["1","2"], ext=["html","zip"]),
       fastp = [setn+"/{sample}/reads_1_trim.fastq.gz",
                setn+"/{sample}/reads_2_trim.fastq.gz",
                setn+"/{sample}/fastp.json",
                setn+"/{sample}/fastp.html"],
       fsatqc2 = expand(setn+"/{{sample}}/reads_{n}_trim_fastqc.{ext}", n=["1","2"], ext=["html","zip"]),
    output:
       S3.remote("eukcontainer/contigs/"+megahit_version+"/preprocessing_reports/{sample}.reports.tar.gz")
    threads: 1
    shell:
        # some manipulations to avoid having files under "set_x" folder in tar file:
        "mkdir -p tmp_reports_{wildcards.sample}/{wildcards.sample} ; "
        "cp {input} tmp_reports_{wildcards.sample}/{wildcards.sample}/ ; "
        "tar -czf {output} -C tmp_reports_{wildcards.sample} $(ls tmp_reports_{wildcards.sample}/) ; "
        "rm -r tmp_reports_{wildcards.sample} ; "





############################
##        Assembly        ##
############################

rule megahit_unfiltered:
    input:
        setn+"/{sample}/reads_1_trim.fastq.gz",
        setn+"/{sample}/reads_2_trim.fastq.gz"
    output:
        directory(setn+"/{sample}/megahit_unfiltered"), #{output[0]}
        setn+"/{sample}/contigs.megahit.fa", #{output[1]} #need to explicitly write in tar operation
    log:
        setn+"/{sample}/log_megahit.txt" #need to explicitly write in tar operation
    conda:
        mainfolder + "/yaml/megahit.yaml"
    threads: 7 #But for Megahit I specify -t 14 .
               # This will allow two megahits run in parallell and make sure all cores are 100% busy all the time, 
               # because with single megahit on all cores they are ~75% occupied on average.
    shell:
        "{{ time megahit -t 14 -1 {input[0]} -2 {input[1]} -o {output[0]} --min-contig-len 500; }} 2> {log}; "
        "cp {output[0]}/final.contigs.fa {output[1]}; " # copy results one folder up for proper QUAST labeling later


rule megahit_upload:
    input:
        setn+"/{sample}/contigs.megahit.fa", #{output[1]} #need to explicitly write in tar operation
        setn+"/{sample}/log_megahit.txt",
    output:
        S3.remote("eukcontainer/contigs/"+megahit_version+"/megahit/{sample}.contigs.megahit.fa.tar.gz"), #{output[2]}
    shell:
        "touch {output} ; " # IMPORTANT! A stamp to reserve the temp local folder,
                            # so no finished tarring processes will delete it before all other tars also finish. 
        "tar -cf {output} -I 'pigz' -C "+setn+" {wildcards.sample}/contigs.megahit.fa {wildcards.sample}/log_megahit.txt; "





############################
##    Contig processing   ##
############################


rule raw_contigs_size_filtering:
    input:
        #get_megahit_source
        S3.remote("eukcontainer/contigs/"+megahit_version+"/megahit/{sample}.contigs.megahit.fa.tar.gz"),
    output:
        setn+"/{sample}/contigs.megahit.fa",             # {output[0]}
        setn+"/{sample}/contigs.megahit.sizefiltered.fa", # {output[1]}
    shell:
        #unpack megahit contigs tar
        "mkdir -p {setn} ; "
        "tar -xf {input} -C {setn} ; " #will give {output[0]}

        # Run sizefiltering
        'awk \'BEGIN {{FS = "\\t" ; OFS = "\\n"}} {{header = $0 ; getline seq ; '
        'if (length(seq) >= '+str(mincontiglen)+') {{print header, seq}}}}\' < {output[0]} > {output[1]}; '
        ## ALWAYS escape { } \ by {{ }} \\ in snakemake, if you want to pass { } \ to a command, such as awk




rule EukRep:
    input:
        setn+"/{sample}/contigs.megahit.sizefiltered.fa"
    output:
        setn+"/{sample}/contigs.EukRep.fa",
        S3.remote("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/eukrep/{sample}.contigs.EukRep.fa.tar.gz"), #{output[1]}
    log:
        setn+"/{sample}/log_EukRep.txt"
    shell:
        "touch {output[1]} ; "
        "{{ time EukRep -i {input} -o {output[0]} -m "+EukRep_mode+" --min "+str(mincontiglen)+"; }} 2> {log} ; "
        "tar -cf {output[1]} -I 'pigz' -C " + setn + 
        " {wildcards.sample}/contigs.EukRep.fa {wildcards.sample}/log_EukRep.txt "
        " {wildcards.sample}/contigs.megahit.sizefiltered.fa"





############################
##  QUAST contig analysis ##
############################


rule QUAST_raw_contigs:
    input:
        #get_megahit_source
        S3.remote("eukcontainer/contigs/"+megahit_version+"/megahit/{sample}.contigs.megahit.fa.tar.gz"),
    params:
        contigs=setn+"/{sample}/contigs.megahit.fa", # intermediate file
        tmp="tmp_quast_raw_{sample}",                # temporary folder
    output:
        directory(setn+"/quasts/quast_raw_{sample}"),   # {output[0]}
        S3.remote("eukcontainer/contigs/"+megahit_version+"/QUAST_raw_contigs/{sample}.quast.raw.tar.gz"), # {output[1]}
    conda:
        mainfolder + "/yaml/quast.yaml"
    threads: 1
    shell:
        #unpack megahit contigs tar
        "mkdir -p {setn} ; "
        "tar -xf {input} -C {setn} ; " # will give intermediate file

        # Run QUAST
        #"time quast -t {threads} --silent -o {output} {input}"
        "quast -t {threads} --silent --min-contig "+str(mincontiglen)+" -o {output[0]} {params.contigs} ; "
        
        # Pack QUAST results
        # some manipulations to avoid having files under "set_x" folder in tar file:
        "mkdir -p {params.tmp}/quasts ; "
        "cp -r {output[0]} {params.tmp}/quasts/ ; "
        "tar -czf {output[1]} -C {params.tmp} $(ls {params.tmp}/) ; "
        "rm -r {params.tmp} ; "

rule QUAST_multisample_raw_contigs_sizefiltered:
    input:
        expand("{setn}/{sample}/contigs.megahit.sizefiltered.fa", sample=config["SAMPLE"],setn=setn)
    output:
        directory(setn+"/quast_multi_filt_raw")
    conda:
        mainfolder + "/yaml/quast.yaml"
    threads: 4
    shell:
        "quast -t {threads} --silent --min-contig "+str(mincontiglen)+" -o {output} {input}; "

rule QUAST_multisample_EukRep:
    input:
        expand("{setn}/{sample}/contigs.EukRep.fa", sample=config["SAMPLE"],setn=setn)
    output:
        directory(setn+"/quast_multi_EukRep")
    conda:
        mainfolder + "/yaml/quast.yaml"
    threads: 4
    shell: 
        #"time quast -t {threads} --silent -o {output} {input}"
        "quast -t {threads} --silent --min-contig "+str(mincontiglen)+" -o {output} {input}"





############################
##         Metaeuk        ##
############################


# Concatenating all EukRep contigs to speedup Metaeuk by running it on a combined input
rule contig_concatenating:
    input:
        S3.remote(expand("eukcontainer/contigs/{megahit_version}/{EukRep_version}_{EukRep_mode}/eukrep/{sample}.contigs.EukRep.fa.tar.gz",
                         sample=config["SAMPLE"],
                         setn=setn,
                         megahit_version=megahit_version,
                         EukRep_version=EukRep_version,
                         EukRep_mode=EukRep_mode))
    output:
        setn+"/EukRep_contigs_renamed_cat.fasta"
    shell:
        "mkdir -p {setn} ;"
        "cat {input} > aggl_archive ; " 
        "cat {input} | tar -xzivf - -C "+setn+ " ; " +
        mainfolder + "/scripts/contig_rename_concat.sh {output} " + 
        ' '.join(expand("{setn}/{sample}/contigs.EukRep.fa", setn=setn, sample=config['SAMPLE']))


# MetaEuk 
rule MetaEuk_easy_predict:
    input:
        contigs = setn+"/EukRep_contigs_renamed_cat.fasta",
        profiles = EUK_PROFILES,
        sample_list = S3.remote("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/"+MetaEuk_version+"/"+MetaEuk_reference+"/"+setn+"_samples.txt"),
    params:
        tmp = "tmp_MetaEuk_"+setn,
        aux_files_folder = setn+"/tempFolder/latest",
        aux_files_subfolder = "tempFolder/latest",
        aux_files_pattern = "{contigs*,MetaEuk_preds.*}",
    log: setn+"/log_MetaEuk_easypredict.txt"
    output:
        fasta = setn+"/MetaEuk_preds.fasta",
        tempfolder = directory(setn+"/tempFolder"),
        tsv = setn+"/MetaEuk_preds.fasta.headersMap.tsv",
        tar = S3.remote("eukcontainer/contigs/"+megahit_version+"/"+EukRep_version+"_"+EukRep_mode+"/"+MetaEuk_version+"/"+MetaEuk_reference+"/"+setn+"_MetaEuk_preds.fasta.tar.gz"),
    threads: maxthreads
    shell:
        #MetaEuk run
        "{{ time metaeuk easy-predict {input.contigs} {input.profiles} {output.fasta} {output.tempfolder}"
        " --metaeuk-eval 0.0001 -e 100 --slice-search --min-ungapped-score 35"
        " --min-length 40 --min-exon-aa 20 --metaeuk-tcov 0.6; }} > {log} 2>&1 ; "

        # Pack MetaEuk results
        "mkdir -p {params.tmp}/{params.aux_files_subfolder} ; "
        "cp {params.aux_files_folder}/{params.aux_files_pattern} {params.tmp}/{params.aux_files_subfolder}/ ; "
        "cp {output.fasta} {output.tsv} {log} {input.sample_list}   {params.tmp}/ ; "
        "tar -czf {output.tar} -C {params.tmp} $(ls {params.tmp}/) ; "
        "rm -r {params.tmp} ; "





############################
##  Final MultiQC report  ##
############################


rule MultiQC:
    input:
        #expand(setn+"/{sample}/{file}", sample=config["SAMPLE"], file=['reads_1_fastqc.html', 'reads_2_fastqc.html', 'reads_1_trim_fastqc.html', 'reads_2_trim_fastqc.html', 'fastp.html']),
        report_tars = S3.remote(expand("eukcontainer/contigs/"+megahit_version+"/preprocessing_reports/{sample}.reports.tar.gz", sample=config["SAMPLE"])),
        quasts_raw = S3.remote(expand("eukcontainer/contigs/"+megahit_version+"/megahit/{sample}.contigs.megahit.fa.tar.gz", sample=config["SAMPLE"])),
        #quasts_raw = expand(setn+"/quasts/quast_{stage}_{sample}", stage=["raw"], sample=config['SAMPLE']),
        #quasts_multi = expand(setn+"/{file}", file=['quast_multi_EukRep', 'quast_multi_filt_raw'])
    output:
        setn+"/multiqc_report.html",
        directory(setn+"/multiqc_data")
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    shell:
        "for i in {input.report_tars}; do  tar -xzf $i -C {setn}; done ; "
        "for i in {input.quasts_raw}; do  tar -xzf $i -C {setn}; done ; "
        "multiqc -d -f -o "+setn+" "+setn+"/"
# -d option makes multiqc analyse all samples with same names. In our case all samples are reads_1/2, so this is crucial
# -f option makes force overwrite of previous multiqc output. Important for Snakemake reruns.
# -o specifies outdir of output files
