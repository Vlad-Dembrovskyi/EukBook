# This is the pipeline to run MetaEuk from beginning (given sample accession) without any kraken filtering and bowtie mapping

# Compared to Snakefile_simple here we wisely concatenate all EukRep contigs and running Metaeuk on all of them together. Should speed up.


## Required config parameters:
# SAMPLE - passed with config file by --configfile samples.yaml or --config SAMPLE=SRR2844600
# SAMPLESET - for creating a subfolder dedicated to certain group of samples, listed in yaml file

## Benchmarking mode?

#benchmark=True
benchmark=False

if (benchmark==False):
    downloading_script="download_fastq.sh"
    mincontiglen=5000
    EUK_PROFILES="/mnt/metaeuk_profiles/MMETSP_uniclust50_MERC_profiles_more_than1"
else:
    downloading_script="download_fastq_short.sh" # subsets only first 10k reads
    mincontiglen=100 #otherwise no contigs left, too few reads for 5000kb+ contigs
    EUK_PROFILES="/mnt/metaeuk_profiles_short/MMETSP_uniclust50_MERC_profiles_more_than1_few"


# hardcoded parameters:
mainfolder='/home/ubuntu/EukBook'
maxthreads = snakemake.utils.available_cpu_count()

# Handling different input ways of SAMPLESET config.
# SAMPLESET should be passed with yaml config file alongside with sample accessions.
# Then it is an array of values, of which we have to take only the first argument  - [0].
# But it can be overwritten with --config SAMPLESET=xxx parameter of snakemake command, if needed.
# Then it is a string itself, not array. Applying [0] to string would return only its first character.
if type(config["SAMPLESET"]) is list:
    setn=config["SAMPLESET"][0]
elif type(config["SAMPLESET"]) is str:
    setn=config["SAMPLESET"]



# Email notifications
onstart:
    print("Workflow finished, no error")
    shell("mail -s 'Snakemake "+setn+" - Workflow started' dembra96@gmail.com < {log}" )

onsuccess:
    print("Workflow finished, no error")
    shell("mail -s 'Snakemake "+setn+" - Workflow successfully finished' dembra96@gmail.com < {log}")

onerror:
    print("An error occurred")
    shell("mail -s 'Snakemake "+setn+" - an error occurred' dembra96@gmail.com < {log}")




################
### Workflow ###
################

rule all:
    input:
     # the samples themselves
        #expand("{setn}/{sample}/reads_1.fastq.gz", sample=config["SAMPLE"], setn=config["SAMPLESET"]),
        #expand("{sample}/"reads_2.fastq.gz", sample=config["SAMPLE"]),
        #expand("{sample}/"reads_1_trim.fastq.gz", sample=config["SAMPLE"]),
        #expand("{sample}/"reads_2_trim.fastq.gz", sample=config["SAMPLE"]),
     # can be ommited in benchmarking   
        #expand(setn+"/{sample}/reads_1_fastqc.html", sample=config["SAMPLE"]),
        #expand(setn+"/{sample}/reads_2_fastqc.html", sample=config["SAMPLE"]), 
        #expand(setn+"/{sample}/reads_1_trim_fastqc.html", sample=config["SAMPLE"]),
        #expand(setn+"/{sample}/reads_2_trim_fastqc.html", sample=config["SAMPLE"]),
     # unfiltered pathway
        #expand("{setn}/{sample}/megahit_unfiltered", sample=config["SAMPLE"], setn=setn),      # --min-contig-len 500
        #expand(setn+"/{sample}/contigs.megahit.fa", sample=config["SAMPLE"]),
        expand(setn+"/{sample}/contigs.megahit.sizefiltered.fa", sample=config["SAMPLE"]),
        #expand("{setn}/{sample}/contigs.EukRep.fa", sample=config["SAMPLE"], setn=setn),   # -m lenient
        #expand("{setn}/{sample}/quast_{sample}", sample=config["SAMPLE"], setn=setn),
        setn+"/quast_multi_raw",
        setn+"/quast_multi_EukRep", # multisample quast for EukRep contigs from all samples. 
        setn+"/quast_multi_filt_raw",
        setn+"/EukRep_contigs_renamed_cat.fasta",
        setn+"/MetaEuk_preds.fasta",
        setn+"/multiqc_report.html",



### Preprocessing
rule download_fastq:
    output: 
        setn+"/{sample}/reads_1.fastq.gz",
        setn+"/{sample}/reads_2.fastq.gz"
    shell: 
        "{mainfolder}/scripts/" + downloading_script + " {wildcards.sample} {output}" 


rule FastQC_untrimmed:
    input:
        setn+"/{sample}/reads_1.fastq.gz",
        setn+"/{sample}/reads_2.fastq.gz"
    output:
        setn+"/{sample}/reads_1_fastqc.html",
        setn+"/{sample}/reads_2_fastqc.html"
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    threads: 2
    shell: 
        "fastqc -t {threads} {input} "

        
rule fastp_trimming:
    input: 
        setn+"/{sample}/reads_1.fastq.gz",
        setn+"/{sample}/reads_2.fastq.gz"
    output: 
        setn+"/{sample}/reads_1_trim.fastq.gz",
        setn+"/{sample}/reads_2_trim.fastq.gz",
        setn+"/{sample}/fastp.json",
        setn+"/{sample}/fastp.html" 
    threads: 4  
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    shell: 
        "fastp -r -w {threads} -i {input[0]} -I {input[1]} -o {output[0]} -O {output[1]} -j {output[2]} -h {output[3]}"


rule FastQC_trimmed:
    input:
        setn+"/{sample}/reads_1_trim.fastq.gz",
        setn+"/{sample}/reads_2_trim.fastq.gz"
    output:
        setn+"/{sample}/reads_1_trim_fastqc.html",
        setn+"/{sample}/reads_2_trim_fastqc.html"
        #output is the input without extentiuon plus "_fastqc.html". And "_fastqc.zip".
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    threads: 2
    shell:
        "fastqc -t {threads} {input} "




### Assembly without filtering

rule megahit_unfiltered:
    input:
        setn+"/{sample}/reads_1_trim.fastq.gz",
        setn+"/{sample}/reads_2_trim.fastq.gz"
    output:
        directory(setn+"/{sample}/megahit_unfiltered"), #{output[0]}
        setn+"/{sample}/contigs.megahit.fa" #{output[1]}
    log:
        setn+"/{sample}/log_megahit.txt"
    conda:
        mainfolder + "/yaml/megahit.yaml"
    threads: int( maxthreads * 0.5 )
#this will allow two megahits run in parallell and make sure all cores are 100% busy al the time, because with single megahit on all cores they are ~75% occupied on average.
    shell:
        #"rm -r "+setn+"/{wildcards.sample}/megahit_unfiltered; "
        "{{ time megahit -t "+str(maxthreads)+" -1 {input[0]} -2 {input[1]} -o {output[0]} --min-contig-len 500; }} 2> {log}; "
        "cp {output[0]}/final.contigs.fa {output[1]}; " # copy results one folder up
        "touch {setn}/{wildcards.sample}/checkpoint_{wildcards.sample}_is_done.txt" 

rule raw_contigs_size_filtering:
    input:
        setn+"/{sample}/contigs.megahit.fa"
    output:
        setn+"/{sample}/contigs.megahit.sizefiltered.fa"
    shell:
        #for fasta:
        'awk \'BEGIN {{FS = "\\t" ; OFS = "\\n"}} {{header = $0 ; getline seq ; if (length(seq) >= '+str(mincontiglen)+') {{print header, seq}}}}\' < {input} > {output}; '
        #for fastq (but I have fasta):
        #'awk \'BEGIN {{FS = "\\t" ; OFS = "\\n"}} {{header = $0 ; getline seq ; getline qheader ; getline qseq ; if (length(seq) >= '+str(mincontiglen)+') {{print header, seq, qheader, qseq}}}}\' < {input} > {output}; '
# ALWAYS escape { } \ by {{ }} \\ in snakemake, if you want to pass { } \ to a command, such as awk 


rule EukRep:
    input:
        setn+"/{sample}/contigs.megahit.sizefiltered.fa"
    output:
        setn+"/{sample}/contigs.EukRep.fa"
    log:
        setn+"/{sample}/log_EukRep.txt"
    shell:
        "{{ time EukRep -i {input} -o {output} -m lenient; }} 2> {log} ; "

# QUAST

rule QUAST_multisample_raw_contigs:
    input:
        expand("{setn}/{sample}/contigs.megahit.fa", sample=config["SAMPLE"],setn=setn)
    output:
        directory(setn+"/quast_multi_raw")
    conda:
        mainfolder + "/yaml/quast.yaml"
    threads: 4
    shell:
        #"time quast -t {threads} --silent -o {output} {input}"
        "quast -t {threads} --silent -o {output} {input}"

rule QUAST_multisample_raw_contigs_sizefiltered:
    input:
        expand("{setn}/{sample}/contigs.megahit.sizefiltered.fa", sample=config["SAMPLE"],setn=setn)
    output:
        directory(setn+"/quast_multi_filt_raw")
    conda:
        mainfolder + "/yaml/quast.yaml"
    threads: 4
    shell:
        "quast -t {threads} --silent -o {output} {input}; "

rule QUAST_multisample_EukRep:
    input:
        expand("{setn}/{sample}/contigs.EukRep.fa", sample=config["SAMPLE"],setn=setn)
    output:
        directory(setn+"/quast_multi_EukRep")
    conda:
        mainfolder + "/yaml/quast.yaml"
    threads: 4
    shell: 
        #"time quast -t {threads} --silent -o {output} {input}"
        "quast -t {threads} --silent -o {output} {input}"



# Concatenating all EukRep contigs
rule contig_concatenating:
    input:
        expand("{setn}/{sample}/contigs.EukRep.fa", sample=config["SAMPLE"],setn=setn)
    output:
        setn+"/EukRep_contigs_renamed_cat.fasta"
    shell:
        mainfolder + "/scripts/contig_rename_concat.sh {output} {input}" 



# MetaEuk 
rule MetaEuk_createdb:
    input:
        setn+"/EukRep_contigs_renamed_cat.fasta"
    output:
        directory(setn+"/contigs_db")
    threads: maxthreads
    shell:
        "mkdir -p {output}; "
        "metaeuk createdb {input} {output}/contigs --dbtype 2"

rule MetaEuk_easy_predict:
    input:
        setn+"/contigs_db", #{input[0]}
        EUK_PROFILES        #{input[1]}
    output:
        setn+"/MetaEuk_preds.fasta",        #{output[0]}
        directory(setn+"/tempFolder") #{output[1]}
    log:
        setn+"/log_MetaEuk_easypredict.txt"
    threads: maxthreads
    resources: mem_mb=120000
    shell:
        "{{ time metaeuk easy-predict {input[0]}/contigs {input[1]} {output[0]} {output[1]}"
        " --metaeuk-eval 0.0001 -e 100 --slice-search --min-ungapped-score 35"
        " --min-length 40 --min-exon-aa 20 --metaeuk-tcov 0.6; }} 2> {log}"

# MultiQC
rule MultiQC:
    input:
        expand(setn+"/{sample}/{file}", sample=config["SAMPLE"], file=['reads_1_fastqc.html', 'reads_2_fastqc.html', 'reads_1_trim_fastqc.html', 'reads_2_trim_fastqc.html', 'fastp.html']),
        expand(setn+"/{file}", file=['quast_multi_raw', 'quast_multi_EukRep', 'quast_multi_filt_raw'])
    output:
        setn+"/multiqc_report.html",
        directory(setn+"/multiqc_data")
    conda:
        mainfolder + "/yaml/conda_packages.yaml"
    shell:
        "multiqc -d -f -o "+setn+" "+setn+"/"
# -d option makes multiqc analyse all samples with same names. In our case all samples are reads_1/2, so this is crucial
# -f option makes force overwrite of previous multiqc output. Important for Snakemake reruns.
# -o specifies outdir of output files
